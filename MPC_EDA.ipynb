{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MPC_EDA.ipynb",
      "provenance": [],
      "mount_file_id": "1HyHUu-ZrfEkYWQNZFPDYP7Dma5ceKiC1",
      "authorship_tag": "ABX9TyNKGJBzIR1F9T8cMsv36jm9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lodi-m/Misinformation-Prone-Communities/blob/main/MPC_EDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHXm_fr96e8G"
      },
      "source": [
        "Tasks to do: \n",
        "\n",
        "* **Further clean both datasets - realizes that left some stuff out which should be cleaned**\n",
        "*   Get list of questions that going to be looking into \n",
        "  * What are the most popular covid-related words? (will need to remove common english words, think there's a library for it, need to look into it)\n",
        "  * are certain words more used in certain times? try and link to covid events in canada\n",
        "  * seperate into fake vs real tweets \n",
        "     * when are more fake/real tweets tweeted? relate to world events\n",
        "  * are there any accounts which were specficially just made for the tweet? look at very close tweet date vs user_created date, if within a day then thats likely?\n",
        "  * \n",
        "  * try looking at some common nlp eda questions too\n",
        "\n",
        "*   Try doing some more complicated data visualization! Look at Omer's EAFP EDA for inspiration? - especially the heatmaps\n",
        "*   Specifically look into NLP related EDA - is there something specific we can do? Maybe get one of those word map thingys that show which words are most common in all the tweets? ONLY DO THIS FOR THE TESTING SET THOUGH\n",
        "  * Don't really care about the most common words in training set? Think more about it though \n",
        "*   \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgP3IZRF0Vng"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-2QNY6utXrK"
      },
      "source": [
        "#### Further Data Cleaning\n",
        "Looking at both datasets once again, I notice that there are some things which need to be cleaned up, especially the `created_at` and `user_created_at` columns in the training dataset. I will also convert the `label` entries to be integers instead of floats in the training dataset.\n",
        "\n",
        "I will first start with the training dataset and then move onto the testing dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QQkRR9qvHQF"
      },
      "source": [
        "#### Cleaning training dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2rSw3ntLKaK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "f085d47d-8001-42fe-f19b-d373ab071a45"
      },
      "source": [
        "training_data = pd.read_csv(\"/content/drive/MyDrive/MPC/notclean_training_data.csv\")\n",
        "training_data"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>created_at</th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>user_created_at</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Mon Apr 27 23:59:45 +0000 2020</td>\n",
              "      <td>1254923219359936512</td>\n",
              "      <td>excellent devastating story anyone who says co...</td>\n",
              "      <td>Wed Jul 01 01:59:30 +0000 2009</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Fri May 01 03:33:46 +0000 2020</td>\n",
              "      <td>1256064242572300293</td>\n",
              "      <td>had a grown man today tell me  he refuses to w...</td>\n",
              "      <td>Tue Oct 04 23:45:47 +0000 2011</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Thu Apr 23 16:39:54 +0000 2020</td>\n",
              "      <td>1253362977828294657</td>\n",
              "      <td>when will you apologize for hawking hydroxycor...</td>\n",
              "      <td>Wed Apr 26 00:38:34 +0000 2017</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Thu Apr 23 04:45:54 +0000 2020</td>\n",
              "      <td>1253183292947550208</td>\n",
              "      <td>isamassiveunderstatement covid  is just like t...</td>\n",
              "      <td>Sat Nov 09 19:28:24 +0000 2019</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Tue Apr 21 16:32:32 +0000 2020</td>\n",
              "      <td>1252636346134355968</td>\n",
              "      <td>just had an encounter with the most ignorant c...</td>\n",
              "      <td>Thu Nov 03 22:36:53 +0000 2016</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19995</th>\n",
              "      <td>Sun Aug 09 07:46:07 +0000 2020</td>\n",
              "      <td>1292366536020127745</td>\n",
              "      <td>what do you thinkpeople who dont take the even...</td>\n",
              "      <td>Tue May 15 10:27:22 +0000 2007</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19996</th>\n",
              "      <td>Wed Apr 01 06:19:35 +0000 2020</td>\n",
              "      <td>1245234333318815744</td>\n",
              "      <td>see that green line at the bottom thats the on...</td>\n",
              "      <td>Sun Jun 21 19:15:21 +0000 2009</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19997</th>\n",
              "      <td>Wed Jul 01 17:23:09 +0000 2020</td>\n",
              "      <td>1278378624392073217</td>\n",
              "      <td>after october when millions join the dole que...</td>\n",
              "      <td>Wed Jul 24 16:13:34 +0000 2013</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19998</th>\n",
              "      <td>Thu Mar 12 19:50:14 +0000 2020</td>\n",
              "      <td>1238190584222961664</td>\n",
              "      <td>to levando pra mim que eles tbm so imunes pld...</td>\n",
              "      <td>Mon Sep 07 17:19:15 +0000 2009</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19999</th>\n",
              "      <td>Sat Mar 28 20:55:04 +0000 2020</td>\n",
              "      <td>1244005108351991810</td>\n",
              "      <td>can i catch covid from my pet who    who conti...</td>\n",
              "      <td>Tue Oct 14 22:09:02 +0000 2008</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>20000 rows Ã— 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                           created_at  ...  label\n",
              "0      Mon Apr 27 23:59:45 +0000 2020  ...    0.0\n",
              "1      Fri May 01 03:33:46 +0000 2020  ...    0.0\n",
              "2      Thu Apr 23 16:39:54 +0000 2020  ...    0.0\n",
              "3      Thu Apr 23 04:45:54 +0000 2020  ...    0.0\n",
              "4      Tue Apr 21 16:32:32 +0000 2020  ...    0.0\n",
              "...                               ...  ...    ...\n",
              "19995  Sun Aug 09 07:46:07 +0000 2020  ...    1.0\n",
              "19996  Wed Apr 01 06:19:35 +0000 2020  ...    1.0\n",
              "19997  Wed Jul 01 17:23:09 +0000 2020  ...    1.0\n",
              "19998  Thu Mar 12 19:50:14 +0000 2020  ...    1.0\n",
              "19999  Sat Mar 28 20:55:04 +0000 2020  ...    1.0\n",
              "\n",
              "[20000 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFon0wV3wlJv"
      },
      "source": [
        "training_data['date_tweet_created'] = training_data.loc[:, 'created_at'].apply(lambda x : x[0:10] + ' ' + x[25:30])\n",
        "training_data['time_tweet_created'] = training_data.loc[:, 'created_at'].apply(lambda x : x[11:20])\n",
        "\n",
        "training_data['date_user_created'] = training_data.loc[:, 'user_created_at'].apply(lambda x : x[0:10] + ' ' + x[25:30])\n",
        "training_data['time_user_created'] = training_data.loc[:, 'user_created_at'].apply(lambda x : x[11:20])\n",
        "training_data = training_data.drop(['created_at', 'user_created_at'], axis=1)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUtYbEIL0DZL"
      },
      "source": [
        "training_data['label'] = training_data['label'].astype('int')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_YPybsKz337"
      },
      "source": [
        "training_data.to_csv('final_training_data.csv', index=False)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdOKBQHx1cpc"
      },
      "source": [
        "#### Cleaning testing dataset\n",
        "\n",
        "Note that I scraped twitter multiple times in order to get my testing data. Here I will be combining all those files and cleaning the final testing dataset as well. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 684
        },
        "id": "mEZ0DNZBt2iW",
        "outputId": "cf9d69ac-034c-4ee4-8660-fe9b094fc63d"
      },
      "source": [
        "testing_data1 = pd.read_csv(\"https://raw.githubusercontent.com/lodi-m/Misinformation-Prone-Communities/main/data/canada_tweets.csv\")\n",
        "testing_data2 = pd.read_csv(\"/content/drive/MyDrive/MPC/canada_tweets_1.csv\")\n",
        "testing_data = pd.concat([testing_data1, testing_data2])\n",
        "testing_data"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>time_accessed</th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>full_text</th>\n",
              "      <th>created_at</th>\n",
              "      <th>geo</th>\n",
              "      <th>place_name</th>\n",
              "      <th>coordinates</th>\n",
              "      <th>user_location</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2021-10-22 15:50</td>\n",
              "      <td>1.450000e+18</td>\n",
              "      <td>Bolsanaro has charges against humanity for all...</td>\n",
              "      <td>2021-10-20 15:04</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Surrey</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Turtle Island</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2021-10-22 15:50</td>\n",
              "      <td>1.450000e+18</td>\n",
              "      <td>Great news: â€œ90% of eligible residents have at...</td>\n",
              "      <td>2021-10-19 11:52</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Ottawa</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Ottawa, Ontario</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2021-10-22 15:50</td>\n",
              "      <td>1.450000e+18</td>\n",
              "      <td>Oregon set to reach herd immunity? \\n\\nhttps:/...</td>\n",
              "      <td>2021-10-18 11:08</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Toronto</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Toronto, and Vancouver Canada</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2021-10-22 15:50</td>\n",
              "      <td>1.450000e+18</td>\n",
              "      <td>@TorontosMayor maybe you can post some empiric...</td>\n",
              "      <td>2021-10-13 23:14</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Ajax</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Ajax, Ontario</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2021-10-22 15:50</td>\n",
              "      <td>1.450000e+18</td>\n",
              "      <td>Get #vaccinated people - everywhere \\nWe need ...</td>\n",
              "      <td>2021-10-22 0:43</td>\n",
              "      <td>NaN</td>\n",
              "      <td>QuÃ©bec</td>\n",
              "      <td>NaN</td>\n",
              "      <td>QuÃ©bec, Canada</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2829</th>\n",
              "      <td>2021-11-21 5:06</td>\n",
              "      <td>1.459050e+18</td>\n",
              "      <td>AUZGMT â€” Indigenous COVID risk as country reop...</td>\n",
              "      <td>2021-11-12 7:03</td>\n",
              "      <td>{'type': 'Point', 'coordinates': [49.28248253,...</td>\n",
              "      <td>Vancouver</td>\n",
              "      <td>{'type': 'Point', 'coordinates': [-123.1272187...</td>\n",
              "      <td>Americas | United Kingdom</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2830</th>\n",
              "      <td>2021-11-21 5:06</td>\n",
              "      <td>1.459050e+18</td>\n",
              "      <td>AUZGMT â€” Push for Indigenous COVID-19 vaccine ...</td>\n",
              "      <td>2021-11-12 7:03</td>\n",
              "      <td>{'type': 'Point', 'coordinates': [49.28248253,...</td>\n",
              "      <td>Vancouver</td>\n",
              "      <td>{'type': 'Point', 'coordinates': [-123.1272187...</td>\n",
              "      <td>Americas | United Kingdom</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2831</th>\n",
              "      <td>2021-11-21 5:06</td>\n",
              "      <td>1.459050e+18</td>\n",
              "      <td>@vera_tenacious @gingerqueenjen @BarryHunt008 ...</td>\n",
              "      <td>2021-11-12 7:01</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Vancouver</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Vancouver</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2832</th>\n",
              "      <td>2021-11-21 5:06</td>\n",
              "      <td>1.459050e+18</td>\n",
              "      <td>@quarantinebeat @vera_tenacious @BarryHunt008 ...</td>\n",
              "      <td>2021-11-12 6:57</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Vancouver</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Vancouver</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2833</th>\n",
              "      <td>2021-11-21 5:06</td>\n",
              "      <td>1.459050e+18</td>\n",
              "      <td>My first time eating in a restaurant. I showed...</td>\n",
              "      <td>2021-11-12 6:49</td>\n",
              "      <td>{'type': 'Point', 'coordinates': [49.25937909,...</td>\n",
              "      <td>Vancouver</td>\n",
              "      <td>{'type': 'Point', 'coordinates': [-123.0696663...</td>\n",
              "      <td>Vancouver BC</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7033 rows Ã— 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         time_accessed  ...                  user_location\n",
              "0     2021-10-22 15:50  ...                  Turtle Island\n",
              "1     2021-10-22 15:50  ...                Ottawa, Ontario\n",
              "2     2021-10-22 15:50  ...  Toronto, and Vancouver Canada\n",
              "3     2021-10-22 15:50  ...                  Ajax, Ontario\n",
              "4     2021-10-22 15:50  ...                 QuÃ©bec, Canada\n",
              "...                ...  ...                            ...\n",
              "2829   2021-11-21 5:06  ...      Americas | United Kingdom\n",
              "2830   2021-11-21 5:06  ...      Americas | United Kingdom\n",
              "2831   2021-11-21 5:06  ...                      Vancouver\n",
              "2832   2021-11-21 5:06  ...                      Vancouver\n",
              "2833   2021-11-21 5:06  ...                   Vancouver BC\n",
              "\n",
              "[7033 rows x 8 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwCm6WbYuxPS",
        "outputId": "5976a1ff-01f8-49b4-8c5c-50cf8d18c297"
      },
      "source": [
        "testing_data['full_text'].duplicated().sum()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "625"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_h2mCwP607pj",
        "outputId": "01802282-3dbe-46b8-a9de-5268d8b9e9f6"
      },
      "source": [
        "testing_data.drop_duplicates(subset=['full_text'], inplace=True, ignore_index=True)\n",
        "testing_data['full_text'].duplicated().sum()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5o7a5M7W6gh6",
        "outputId": "530b6420-7cab-426c-fa06-5dca5a6d01bb"
      },
      "source": [
        "testing_data['text'] = testing_data['full_text'].str.lower().str.replace('rt|@[^\\s]+|[^A-Za-z ]', '', regex=True)\n",
        "testing_data['text'] = testing_data['full_text'].str.lower().str.replace('http\\S+', '', regex=True) \n",
        "testing_data['text'].duplicated().sum()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAblf_MWvagY"
      },
      "source": [
        "### Exploratory Data Analysis \n",
        "\n",
        "As I am working with two seperate datasets, I will be doing EDA for them seperately to keep everything more organized. I will first start with the training dataset and then move onto the testing dataset. All results from both datasets will be summarized and made available at the bottom of this notebook, as well as in a seperate markdown file in the repo. \n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNAKOFDtv5Rp"
      },
      "source": [
        "#### EDA - Training Dataset\n",
        "\n",
        "The training dataset I am using is from the CoAID (<ins>Co</ins>vid-19 he<ins>A</ins>lthcare m<ins>I</ins>sinformation <ins>D</ins>ataset). You can find the dataset at this repo https://github.com/cuilimeng/CoAID. You can also find more information about how exactly the dataset was created in the research paper located at https://arxiv.org/abs/2006.00885."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTuwXDctxjOX"
      },
      "source": [
        "#### EDA - Testing Dataset\n",
        "\n",
        "I created the testing dataset by using the Twitter API. I collected tweets which contained COVID-19 related keywords and were tweeted from anywhere in Canada after January 1st 2019. I disregarded any retweets as well.\n",
        "\n",
        "I looked at for the following keywords: \n",
        "pandemic, covid-19, covid19, covid, vaccine, coronavirus, corona, covidvaccine, mask, facemask, immunization, herd immunity, face mask\n",
        "\n",
        "You can see exactly how I created the testing dataset at this notebook: https://github.com/lodi-m/Misinformation-Prone-Communities/blob/main/tweet_farming.ipynb "
      ]
    }
  ]
}